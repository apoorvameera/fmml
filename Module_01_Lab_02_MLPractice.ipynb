{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvameera/fmml/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245bb03a-d2ae-4716-c35e-237eac6596dc"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c390eac5-90af-4484-b89c-c93468cb6f63"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583f7e6b-33d6-4671-cc29-39b38c2c2958"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7264229-477d-4320-878c-97fbabde5ac9"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc5add8-13d4-4bf2-cfe4-0f8b006523e1"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e302dd18-4d33-44f7-e88e-6ff4ab2b15db"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The accuracy of the validation set is affected by the percentage of data allocated to it in your training process. The validation set is a critical part of machine learning model development, as it helps you assess how well your model generalizes to unseen data. Here's how increasing or reducing the percentage of the validation set can impact accuracy:\n",
        "\n",
        "1. **Increasing the Percentage of the Validation Set:**\n",
        "   - **Pros:**\n",
        "     - A larger validation set can provide a more reliable estimate of your model's performance. With more data in the validation set, you get a better sense of how well your model is likely to perform on new, unseen data.\n",
        "     - It can help reduce overfitting, as your model has less data for training, making it more challenging for it to fit noise in the training data.\n",
        "\n",
        "   - **Cons:**\n",
        "     - If you allocate too much data to the validation set, you might not have enough data for training. This can lead to underfitting, where your model may not capture the underlying patterns in the data.\n",
        "\n",
        "2. **Reducing the Percentage of the Validation Set:**\n",
        "   - **Pros:**\n",
        "     - With a smaller validation set, you have more data available for training, which can help your model learn the underlying patterns in the data better.\n",
        "\n",
        "   - **Cons:**\n",
        "     - A small validation set may not provide a robust estimate of your model's generalization performance. It can lead to overfitting, where the model performs well on the validation set but poorly on new data.\n",
        "\n",
        "The choice of the validation set size is a trade-off. You want to strike a balance between having enough data to evaluate your model reliably and having enough data for your model to learn from during training. A common practice is to allocate around 70-80% of the data for training and the remaining 20-30% for validation. However, the specific split may vary depending on the size of your dataset and the nature of your problem.\n",
        "\n",
        "It's also essential to use techniques like cross-validation when the dataset is limited. Cross-validation involves splitting the data into multiple subsets and training/validating the model on each subset. This helps provide a more robust estimate of model performance.\n",
        "\n",
        "In summary, the accuracy of the validation set is influenced by the percentage of data allocated to it, and the optimal percentage depends on the specific problem and dataset. Balancing the trade-off between model evaluation and model learning is key to developing a robust machine learning model."
      ],
      "metadata": {
        "id": "V3PqgbEggKnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.The size of the training and validation sets can have a significant impact on how well you can predict the accuracy of your model on the test set using the validation set. The validation set plays a crucial role in assessing your model's performance and making predictions about its generalization to unseen data (the test set). Here's how the sizes of these sets can affect your ability to predict accuracy on the test set:\n",
        "\n",
        "1. **Large Training Set, Small Validation Set:**\n",
        "   - In this scenario, you have a substantial amount of data for training your model, which can help it learn the underlying patterns in the data effectively.\n",
        "   - However, the small validation set may not provide a reliable estimate of how well your model will generalize to the test set. This is because the validation set may not represent the diversity of data in the test set adequately.\n",
        "\n",
        "   - If the validation set is too small, it might not be representative of the test set, and your accuracy estimate could be overly optimistic. You may miss overfitting issues that could become apparent when evaluated on a larger validation or test set.\n",
        "\n",
        "2. **Small Training Set, Large Validation Set:**\n",
        "   - With a small training set, your model may not learn the underlying patterns well, and it might underfit the data.\n",
        "   - A large validation set provides a more reliable estimate of model performance on the validation data. However, if the training set is too small, your model's performance may not improve significantly.\n",
        "\n",
        "   - While you have a better accuracy estimate on the validation set, your model may not perform well on the test set due to inadequate training.\n",
        "\n",
        "3. **Balanced Training and Validation Set Sizes:**\n",
        "   - A common practice is to allocate a significant portion of the data to the training set (e.g., 70-80%) and the remainder to the validation set.\n",
        "   - This balanced approach allows your model to learn from a sufficiently large training set while providing a reasonably representative validation set.\n",
        "   - It strikes a balance between model learning and evaluation, helping you obtain a more accurate estimate of the model's generalization performance on the test set.\n",
        "\n",
        "In summary, the size of the training and validation sets is crucial for accurate predictions of model performance on the test set. A balanced approach, where you allocate a substantial portion of data to training while retaining a representative validation set, is often a good strategy. The validation set should be large enough to provide a reliable estimate of your model's performance, but not so large that it compromises the training process. This balance will help you make more accurate predictions about your model's accuracy on the test set."
      ],
      "metadata": {
        "id": "y8s_an87gwQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.The percentage of data to reserve for the validation set depends on the size of your dataset and the nature of your machine learning problem. There is no one-size-fits-all answer, but a common practice is to allocate around 20-30% of your data for the validation set. This percentage is often used as a starting point, but adjustments may be necessary based on the following considerations:\n",
        "\n",
        "1. **Size of the Dataset:**\n",
        "   - If you have a large dataset (thousands or millions of data points), you can afford to reserve a smaller percentage for validation. In such cases, 20% might be sufficient.\n",
        "\n",
        "2. **Complexity of the Model:**\n",
        "   - If you are training a complex model with a large number of parameters, it might be more prone to overfitting. In such cases, a larger validation set (closer to 30%) can help provide a more robust estimate of model performance.\n",
        "\n",
        "3. **Availability of Data:**\n",
        "   - If your dataset is small, you may want to allocate a larger portion to the validation set to ensure a representative sample for evaluation.\n",
        "\n",
        "4. **Cross-Validation:**\n",
        "   - If you have a very limited dataset, you can use techniques like k-fold cross-validation. This involves splitting your data into k subsets and training/validating your model on each subset. This way, you can use all your data for validation without sacrificing too much for training.\n",
        "\n",
        "5. **Domain and Problem Specifics:**\n",
        "   - Consider the characteristics of your data and the specific problem you're trying to solve. Some domains or problems may benefit from a larger or smaller validation set based on data distribution and complexity.\n",
        "\n",
        "In practice, it's often beneficial to experiment with different validation set percentages and observe how they affect your model's performance. You can perform multiple training runs with different validation set sizes and choose the one that leads to the best overall performance and generalization.\n",
        "\n",
        "The key is to strike a balance between having enough data for training your model effectively and having a sufficiently large validation set to estimate performance accurately. It's also important to remember that the validation set should be kept separate from the test set, and it should not be used for model tuning or hyperparameter optimization to avoid data leakage."
      ],
      "metadata": {
        "id": "6SsZ8Iy1hP9a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726ae6d1-8237-4460-95b3-9585e1dce619"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "1.This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Yes, averaging the validation accuracy across multiple splits in cross-validation can provide more consistent and reliable results compared to using a single validation set. Cross-validation techniques like k-fold cross-validation and leave-one-out cross-validation are designed to assess a model's performance by repeatedly splitting the data into different subsets for training and validation. Here's why averaging the validation accuracy across these multiple splits is beneficial:\n",
        "\n",
        "1. **Reduced Variance:** Averaging the validation accuracy over multiple folds or splits helps reduce the impact of random variations in a single validation set. Different validation sets may contain different data points, and their performance estimates can vary. Averaging smooths out these variations, giving a more stable and representative measure of your model's performance.\n",
        "\n",
        "2. **Better Generalization Assessment:** Cross-validation provides a more robust estimate of how well your model generalizes to unseen data. By assessing performance across different validation sets, you get a more comprehensive view of how the model will perform in real-world scenarios.\n",
        "\n",
        "3. **Use of All Data for Validation:** Cross-validation ensures that every data point is used for validation in at least one fold. This maximizes the utilization of your data for performance evaluation while avoiding bias introduced by a single static validation set.\n",
        "\n",
        "4. **Model Assessment:** Averaging the results from multiple splits helps provide a more accurate assessment of your model's performance, which is valuable when comparing different models or when tuning hyperparameters.\n",
        "\n",
        "5. **Model Robustness:** By testing the model's performance on multiple validation sets, you can identify if the model's accuracy is consistent across different subsets of the data. This can help you assess model robustness.\n",
        "\n",
        "In summary, averaging the validation accuracy across multiple splits in cross-validation is a good practice for obtaining more consistent and reliable performance estimates for your machine learning model. It helps mitigate the variability associated with a single validation set and provides a more comprehensive evaluation of your model's generalization capabilities."
      ],
      "metadata": {
        "id": "VeDcq5aUhlPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Cross-validation, which involves averaging the validation accuracy across multiple splits of your data, provides a more accurate estimate of how well your model is likely to perform on unseen data (test accuracy). However, it's important to clarify the distinction between validation accuracy and test accuracy:\n",
        "\n",
        "1. **Validation Accuracy:** Validation accuracy is an estimate of how well your model performs on a dataset that it hasn't seen during training. This dataset is part of your original data but is reserved for model evaluation and hyperparameter tuning. Validation accuracy is used to make decisions about model selection and configuration.\n",
        "\n",
        "2. **Test Accuracy:** Test accuracy is a measure of how well your model generalizes to completely new, unseen data. It's typically assessed using a separate dataset that the model has never encountered before, and it serves as the final evaluation of your model's performance.\n",
        "\n",
        "Here's how cross-validation contributes to obtaining a more accurate estimate of test accuracy:\n",
        "\n",
        "- Cross-validation provides a more robust and reliable estimate of how well your model is likely to generalize to new data. By training and validating the model on multiple subsets of your dataset, it helps account for data variability and reduces the risk of overfitting to a single validation set.\n",
        "\n",
        "- While cross-validation can provide a more accurate estimate of how well your model will perform on the test data, it's important to note that it's still an estimate. The actual test accuracy on entirely new, unseen data may vary, but cross-validation is a strong indicator of your model's generalization capabilities.\n",
        "\n",
        "- Cross-validation is a valuable tool for model selection and hyperparameter tuning. When you've chosen a model and its configuration based on cross-validation results, you can then assess its performance on the test data to get a final, realistic measure of how well it will perform in a real-world setting.\n",
        "\n",
        "In summary, while cross-validation doesn't directly give you the exact test accuracy, it provides a more accurate estimate of your model's generalization performance compared to using a single validation set. It's a crucial step in the model development process and helps you make more informed decisions about your model's likely performance on unseen data. The test accuracy, obtained using a completely independent dataset, is the final measure of your model's real-world performance."
      ],
      "metadata": {
        "id": "YpY44yGXh6XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZnDwSMlYiH9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.The number of iterations, or folds, in cross-validation can impact the quality of your estimate. Cross-validation, such as k-fold cross-validation, involves splitting your dataset into multiple subsets, training and validating your model multiple times. Here's how the number of iterations affects the estimate:\n",
        "\n",
        "1. **More Iterations (Higher k):**\n",
        "   - Increasing the number of iterations (higher k) in cross-validation can provide a more reliable estimate of your model's performance. With more folds, you obtain a more comprehensive assessment of how well your model generalizes to different data subsets.\n",
        "\n",
        "   - A larger k reduces the impact of random variations in the validation sets, resulting in a more stable and representative performance estimate.\n",
        "\n",
        "   - You also make more efficient use of your data for validation, as each data point is used for validation in multiple folds. This maximizes the data's contribution to the performance estimate.\n",
        "\n",
        "2. **Fewer Iterations (Lower k):**\n",
        "   - Using a smaller number of iterations in cross-validation can still provide a reasonable estimate of model performance, but it may be more susceptible to variations in the validation set composition.\n",
        "\n",
        "   - With a smaller k, each fold represents a larger portion of the data, and the estimate may be influenced more by the specific composition of the validation sets in those folds.\n",
        "\n",
        "In general, using a higher number of iterations (higher k) in cross-validation tends to provide a better estimate of your model's performance, as it offers a more robust and stable assessment of generalization. However, there are trade-offs to consider:\n",
        "\n",
        "- As k increases, the computational cost of cross-validation also goes up, as you train and validate the model multiple times. This can be a concern if you have limited computational resources.\n",
        "\n",
        "- In cases where you have a small dataset, very high k values may result in each fold having extremely limited data for training and validation, which can lead to less accurate estimates.\n",
        "\n",
        "- A common choice for k in k-fold cross-validation is often 5 or 10, which strikes a balance between computational cost and accuracy of the estimate. However, the choice of k can vary based on your specific dataset and computational resources.\n",
        "\n",
        "In summary, increasing the number of iterations (higher k) in cross-validation generally leads to a better estimate of your model's performance, but the choice of k should consider trade-offs between accuracy, computational resources, and the dataset size. It's essential to choose a value of k that aligns with the specific needs and constraints of your project."
      ],
      "metadata": {
        "id": "W8pH077bidu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Increasing the number of iterations in cross-validation can help mitigate the impact of having a very small training dataset or validation dataset to some extent, but it doesn't entirely replace the need for a sufficiently sized dataset for training and validation. Here's how increasing iterations affects dealing with small datasets:\n",
        "\n",
        "1. **Dealing with a Very Small Training Dataset:**\n",
        "   - If your training dataset is very small, increasing the number of iterations can be helpful because it allows you to utilize the available training data more effectively.\n",
        "   - With more iterations, the model will be trained on different subsets of the small training data, which can help it capture a broader range of patterns.\n",
        "   - However, it's important to note that having a very small training dataset remains a limitation. Cross-validation can only extract the maximum value from the available data but can't generate more data.\n",
        "\n",
        "2. **Dealing with a Very Small Validation Dataset:**\n",
        "   - If your validation dataset is very small, increasing the number of iterations can also be beneficial. More iterations reduce the risk of overfitting to the specific validation data in each fold, providing a more robust estimate of model performance.\n",
        "   - With a small validation dataset, it's more likely to experience variability in the performance estimates, which can be smoothed out with more iterations.\n",
        "\n",
        "While increasing the number of iterations can help in both cases, it's essential to consider the limitations:\n",
        "\n",
        "- If your training dataset is extremely small, the model may struggle to generalize well, even with cross-validation. The limited data may result in high variability in the model's performance.\n",
        "\n",
        "- If your validation dataset is tiny, it can still limit the precision of the performance estimates, especially for model selection or hyperparameter tuning.\n",
        "\n",
        "In situations with very small datasets, it's crucial to acknowledge these limitations and explore other techniques to supplement your data, such as data augmentation, transfer learning, or domain-specific knowledge. Additionally, make the most of the data you have by using techniques like cross-validation to extract the best possible performance estimates."
      ],
      "metadata": {
        "id": "IDqxVIbeix8-"
      }
    }
  ]
}